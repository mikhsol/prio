# 0.2.5 LLM Selection Recommendation

**Task**: Write LLM selection recommendation with rule-based fallback  
**Owner**: Android Developer  
**Status**: ✅ Complete  
**Date**: February 03, 2026

---

## Executive Summary

Based on comprehensive benchmarking on Pixel 9a, we recommend **Phi-3-mini-4k-instruct (Q4_K_M)** as the primary model, with a **mandatory rule-based classifier** for all devices. Critical finding: **The LLM alone achieves only 40% accuracy** on Eisenhower classification, requiring significant prompt engineering improvements or a hybrid approach.

---

## Recommendation Summary

| Component | Recommendation | Confidence |
|-----------|----------------|------------|
| **Primary Model** | Phi-3-mini Q4_K_M | High |
| **Quantization** | Q4_K (5 bits/weight) | High |
| **Fallback** | Rule-based classifier | Critical |
| **Strategy** | Hybrid: Rule-based first, LLM for edge cases | High |

---

## Model Evaluation Results

### Models Considered

| Model | Size | Params | Quality | Speed | Availability |
|-------|------|--------|---------|-------|--------------|
| **Phi-3-mini** | 2.3 GB | 3.8B | ⭐⭐⭐⭐ | ⭐⭐⭐ | ✅ Open |
| Gemma 2B | 1.5 GB | 2B | ⭐⭐⭐ | ⭐⭐⭐⭐ | ✅ Open |
| TinyLlama | 0.8 GB | 1.1B | ⭐⭐ | ⭐⭐⭐⭐⭐ | ✅ Open |
| Llama 3.2 1B | 0.9 GB | 1.2B | ⭐⭐⭐ | ⭐⭐⭐⭐ | ✅ Open |
| Mistral 7B | 4.1 GB | 7B | ⭐⭐⭐⭐⭐ | ⭐⭐ | ⚠️ Large |

### Why Phi-3-mini?

1. **Best quality/size ratio** for instruction-following
2. **Microsoft's optimizations** for mobile deployment
3. **MIT license** - no restrictions
4. **Proven performance** on reasoning tasks
5. **Active community** support

---

## Benchmark Results (Real Device)

### Performance Metrics (Pixel 9a)

| Metric | Result | Target | Status |
|--------|--------|--------|--------|
| Model Load | 1.5 seconds | <5s | ✅ |
| Prompt Processing | 20-22 t/s | >10 t/s | ✅ |
| Token Generation | 4.5 t/s | >3 t/s | ✅ |
| Classification Time | 2-3 seconds | <5s | ✅ |
| RAM Usage | 3.5 GB | <4 GB | ✅ |

### Accuracy Results (Critical Issue)

| Classifier | Accuracy | Notes |
|------------|----------|-------|
| LLM (simple prompt) | **40%** | 8/20 correct |
| Rule-based | **75%** | 15/20 correct (unit tests) |
| Hybrid (recommended) | **85%+** | Estimated |

#### LLM Accuracy by Quadrant

| Quadrant | Correct | Total | Accuracy |
|----------|---------|-------|----------|
| DO (Urgent+Important) | 1 | 5 | 20% |
| SCHEDULE (Important) | 2 | 5 | 40% |
| DELEGATE (Urgent only) | 1 | 5 | 20% |
| ELIMINATE (Neither) | 4 | 5 | 80% |
| **Total** | **8** | **20** | **40%** |

### Root Cause Analysis

The low LLM accuracy is due to:
1. **Simple prompt format** - Not enough context for model
2. **Eisenhower concept not well-encoded** - Model defaults to ELIMINATE bias
3. **Urgency detection weak** - Model struggles with temporal context
4. **Few-shot contamination** - Examples influence output parsing

---

## Recommended Strategy: Hybrid Approach

### Architecture

```
┌─────────────────────────────────────────────────────────────┐
│                     Hybrid Classifier                        │
├─────────────────────────────────────────────────────────────┤
│                                                              │
│  Input: Task text                                           │
│         │                                                   │
│         ▼                                                   │
│  ┌─────────────────────────────────────────┐               │
│  │         Rule-Based Classifier           │               │
│  │  • Keyword matching (urgent, deadline)  │               │
│  │  • Pattern recognition                  │               │
│  │  • High confidence = use directly       │               │
│  └──────────────────┬──────────────────────┘               │
│                     │                                       │
│         ┌───────────┴───────────┐                          │
│         │                       │                          │
│    Confidence                Confidence                    │
│    ≥ 0.75                    < 0.75                       │
│         │                       │                          │
│         ▼                       ▼                          │
│  ┌─────────────┐      ┌─────────────────────┐             │
│  │ Use Result  │      │   LLM Classification │             │
│  │  Directly   │      │   (if available)     │             │
│  └─────────────┘      └──────────┬──────────┘             │
│                                  │                         │
│                    ┌─────────────┴─────────────┐          │
│                    │                           │          │
│               LLM Available              LLM Unavailable  │
│                    │                           │          │
│                    ▼                           ▼          │
│            ┌─────────────┐           ┌─────────────┐     │
│            │ Use LLM     │           │ Use Rule    │     │
│            │ Result      │           │ Result      │     │
│            └─────────────┘           └─────────────┘     │
│                                                           │
└───────────────────────────────────────────────────────────┘
```

### Decision Logic

```kotlin
fun classify(taskText: String): ClassificationResult {
    // Step 1: Always try rule-based first
    val ruleResult = RuleBasedClassifier.classify(taskText)
    
    // Step 2: If high confidence, use directly
    if (ruleResult.confidence >= 0.75) {
        return ruleResult
    }
    
    // Step 3: If LLM available and low confidence, consult LLM
    if (llamaEngine.isLoaded && ruleResult.confidence < 0.75) {
        val llmResult = llamaEngine.classify(taskText)
        
        // Use LLM if it's confident
        if (llmResult.confidence >= 0.7) {
            return llmResult
        }
    }
    
    // Step 4: Fallback to rule-based
    return ruleResult
}
```

---

## Prompt Engineering Recommendations

### Current Prompt (40% accuracy)
```
Classify tasks using Eisenhower Matrix. Answer ONE word: DO, SCHEDULE, DELEGATE, ELIMINATE.
Task: {task}
```

### Recommended Prompt (Estimated 70%+ accuracy)
```
<|user|>You are an expert at the Eisenhower Matrix for task prioritization.

QUADRANTS:
- DO: Urgent AND Important (crisis, deadlines today, fires to put out)
- SCHEDULE: Important but NOT Urgent (planning, learning, health, relationships)
- DELEGATE: Urgent but NOT Important (routine requests, some meetings, admin tasks)
- ELIMINATE: Not Urgent AND Not Important (distractions, time wasters, busywork)

TASK: "{task}"

Classify this task. Consider:
1. Is there a deadline within 24 hours? → Likely DO or DELEGATE
2. Does it contribute to long-term goals? → Important (DO or SCHEDULE)
3. Could someone else do this? → DELEGATE
4. Is it optional with no real impact? → ELIMINATE

Answer with JSON: {"quadrant": "DO|SCHEDULE|DELEGATE|ELIMINATE", "reasoning": "..."}<|end|>
<|assistant|>
```

### Prompt Improvements to Test

1. **Chain-of-thought**: Ask model to reason step-by-step
2. **Structured output**: JSON format for parsing
3. **Explicit definitions**: Define each quadrant clearly
4. **Negative examples**: What each quadrant is NOT
5. **Context clues**: Ask about deadlines, importance, delegation

---

## Implementation Recommendations

### Phase 1: MVP (Now)

1. **Use rule-based classifier as primary**
   - 75% accuracy is acceptable for MVP
   - Zero latency, works offline
   - No RAM requirements

2. **LLM as enhancement** (optional)
   - Available on 6GB+ devices
   - Used for ambiguous cases only
   - Can be disabled by user

### Phase 2: Improvement (Post-MVP)

1. **Improve prompts** with structured output
2. **Collect user corrections** for fine-tuning dataset
3. **A/B test** rule-based vs hybrid approaches
4. **Consider fine-tuning** with 500+ labeled examples

### Phase 3: Scale

1. **LoRA fine-tuning** on collected data
2. **Model distillation** from larger models
3. **Cloud LLM fallback** for complex cases

---

## Fallback Strategy

### Fallback Triggers

| Condition | Action |
|-----------|--------|
| RAM < 6GB | Rule-based only |
| LLM load fails | Rule-based fallback |
| LLM inference timeout (>10s) | Rule-based fallback |
| LLM low confidence (<0.5) | Rule-based result |
| OOM during inference | Unload model, rule-based |

### Rule-Based Classifier Patterns

**High-priority patterns (implemented):**

```kotlin
// Urgency indicators
val urgentPatterns = listOf(
    "urgent", "asap", "immediately", "emergency",
    "deadline today", "due today", "by end of day",
    "server down", "critical", "crisis"
)

// Importance indicators  
val importantPatterns = listOf(
    "project", "client", "customer", "goal",
    "career", "health", "strategy", "plan"
)

// Delegation indicators
val delegatePatterns = listOf(
    "routine", "order supplies", "schedule meeting",
    "status report", "survey", "form"
)

// Elimination indicators
val eliminatePatterns = listOf(
    "social media", "youtube", "browse", "optional",
    "someday", "maybe", "if time permits"
)
```

---

## Risk Assessment

| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|------------|
| LLM accuracy too low | High | Medium | Hybrid approach |
| Model too slow | Low | High | Optimized build |
| OOM on mid-range devices | Medium | High | Aggressive unloading |
| User frustration with errors | Medium | High | Allow manual override |
| Battery drain | Medium | Medium | Background processing limits |

---

## Conclusion

### Final Recommendation

**Use Phi-3-mini Q4_K_M with a hybrid rule-based + LLM approach.**

The LLM alone (40% accuracy) does not meet the >80% target. However, combining rule-based (75% accuracy) with LLM for edge cases should achieve 85%+ accuracy while maintaining fast, offline-capable classification.

### Next Steps

1. ✅ Implement rule-based classifier (done)
2. ✅ Integrate llama.cpp for LLM inference (done)
3. ⏳ Improve LLM prompts with structured output
4. ⏳ Implement hybrid decision logic
5. ⏳ A/B test accuracy in production

---

*Generated: February 03, 2026*  
*Model: Phi-3-mini-4k-instruct Q4_K_M*  
*Test Device: Pixel 9a (Tensor G4, 8GB RAM)*
