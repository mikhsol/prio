# Milestone 0.2.6: Prompt Engineering Implementation

**Date:** February 3, 2026  
**Status:** Implementation Complete, Benchmark Pending  
**Device:** Google Pixel 9a (Tensor G4, 8GB RAM, Android 16)  
**Model:** Phi-3-mini-4k-instruct (Q4_K_M quantization, 2.3 GB)

## Overview

This milestone implements Option A from the Action Plan: **Improve prompt engineering to boost accuracy** for Eisenhower Matrix task classification.

## Implementation Summary

### 0.2.6.1: JSON Structured Output Prompt

Created `PromptStrategy.JSON_STRUCTURED` that forces the model to output in a consistent JSON format:

```kotlin
enum class PromptStrategy {
    JSON_STRUCTURED,
    // ...
}
```

**Prompt Template:**
```
<|user|>
Classify this task into the Eisenhower Matrix.

Task: "{{TASK}}"

Respond ONLY with valid JSON in this exact format:
{"quadrant": "DO|SCHEDULE|DELEGATE|ELIMINATE", "confidence": 0.0-1.0, "reasoning": "brief explanation"}
<|end|>
<|assistant|>
```

### 0.2.6.2: Chain-of-Thought Reasoning

Implemented `PromptStrategy.CHAIN_OF_THOUGHT` that asks the model to reason step-by-step:

**Key Elements:**
- Step 1: Analyze urgency indicators
- Step 2: Assess importance to goals
- Step 3: Consider delegation potential
- Step 4: Synthesize into final classification

### 0.2.6.3: Few-Shot Learning

Created `PromptStrategy.FEW_SHOT` with 4 example classifications (one per quadrant):

```
Example 1: "Server is down, customers affected" → DO (Urgent + Important)
Example 2: "Plan quarterly marketing strategy" → SCHEDULE (Important, not urgent)
Example 3: "Order office supplies" → DELEGATE (Urgent, not important)
Example 4: "Browse social media" → ELIMINATE (Neither)
```

### 0.2.6.4: Expert Persona Prompt

Implemented `PromptStrategy.EXPERT_PERSONA`:

```
You are a senior productivity consultant with 20 years of experience 
implementing the Eisenhower Matrix for Fortune 500 executives...
```

### 0.2.6.5: Combined Optimal Strategy

Created `PromptStrategy.COMBINED_OPTIMAL` that combines:
- Expert persona framing
- Clear quadrant definitions with examples
- JSON structured output requirement
- Confidence scoring

## Technical Implementation

### Files Created

| File | Purpose |
|------|---------|
| `PromptStrategy.kt` | 6 prompt strategies with Phi-3 template formatting |
| `ExtendedTestDataset.kt` | 50 diverse test cases, ~12-13 per quadrant |
| `PromptEngineeringBenchmark.kt` | Comprehensive benchmark runner |
| `PromptStrategyTest.kt` | Unit tests for prompt generation |
| `PromptEngineeringInstrumentedTest.kt` | Device instrumentation tests |

### llama.cpp Integration

Successfully integrated latest llama.cpp (Feb 2026) with:
- ARM64 NEON optimizations
- 4-thread parallel processing
- 2048 token context window
- Q4_K_M quantization support

**Build Configuration:**
- NDK 25.1.8937393
- CMake 3.22.1
- Flag: `-fno-finite-math-only` (required by llama.cpp)

### Model Loading Results

```
Model: Phi-3-mini-4k-instruct-q4.gguf
Load time: 1,927 ms
Implementation: Real (not stub)
Context: 2048 tokens
Threads: 4
```

## Test Dataset

Created 50 test cases with balanced distribution:

| Quadrant | Count | Examples |
|----------|-------|----------|
| DO | 13 | Server crashes, client deadlines, emergencies |
| SCHEDULE | 12 | Strategy planning, skill development, goals |
| DELEGATE | 13 | Routine tasks, status reports, coordination |
| ELIMINATE | 12 | Time wasters, distractions, optional activities |

### Edge Cases Included:
- Ambiguous urgency signals
- Implicit importance
- Cultural/contextual variations
- Tasks requiring domain knowledge

## Running the Benchmark

### Prerequisites

1. **Model file on device:**
   ```bash
   adb push Phi-3-mini-4k-instruct-q4.gguf /data/local/tmp/
   ```

2. **Install app and copy model:**
   ```bash
   ./gradlew installDebug
   adb shell "am start -n app.jeeves.llmtest/.MainActivity"
   adb shell "cat /data/local/tmp/Phi-3-mini-4k-instruct-q4.gguf | \
     run-as app.jeeves.llmtest sh -c 'cat > /data/data/app.jeeves.llmtest/files/model.gguf'"
   ```

3. **Run benchmark:**
   ```bash
   ./gradlew connectedDebugAndroidTest \
     -Pandroid.testInstrumentationRunnerArguments.class=app.jeeves.llmtest.PromptEngineeringInstrumentedTest
   ```

## Expected Results

Based on prior benchmarks (0.2.3) with baseline prompts achieving ~45% accuracy:

| Strategy | Expected Accuracy | Improvement |
|----------|------------------|-------------|
| Baseline | 45-50% | - |
| JSON Structured | 55-60% | +10-15% |
| Chain-of-Thought | 60-65% | +15-20% |
| Few-Shot | 65-70% | +20-25% |
| Expert Persona | 60-65% | +15-20% |
| Combined Optimal | 70-75% | +25-30% |

**Target:** 70% accuracy (≥80% is excellent)

## Metrics Collected

Each strategy benchmark records:
- Accuracy (overall and per-quadrant)
- Precision and Recall
- Average inference time (ms)
- Tokens per second
- Total benchmark duration

## Next Steps

1. Complete benchmark execution on device
2. Analyze results and identify best strategy
3. Document findings and recommendations
4. Update 0.2.6 tasks in ACTION_PLAN.md

## Files Modified

- `CMakeLists.txt` - llama.cpp integration
- `build.gradle.kts` - Native build configuration
- `llama_jni.cpp` - Updated for new llama.cpp API
- `AndroidManifest.xml` - Storage permissions
- `LlmInstrumentedTest.kt` - Updated for real model testing

## References

- [Phi-3 Technical Report](https://arxiv.org/abs/2404.14219)
- [Eisenhower Matrix](https://www.eisenhower.me/eisenhower-matrix/)
- [llama.cpp Project](https://github.com/ggerganov/llama.cpp)
